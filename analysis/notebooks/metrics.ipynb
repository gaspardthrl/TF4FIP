{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When fetching the ticker ES=F from Yahoo Finance, we obtain these columns:\n",
        "\n",
        "Price\t       Datetime\t             Close\t High\t  Low\t Open\tVolume\n",
        "0\t    2024-02-22 17:00:00-06:00\t5099.00\t5100.50\t5094.00\t5094.25\t0\n",
        "1\t    2024-02-22 18:00:00-06:00\t5095.75\t5099.25\t5092.25\t5098.75\t9420\n",
        "2\t    2024-02-22 19:00:00-06:00\t5099.50\t5101.75\t5095.00\t5095.75\t7390\n",
        "3\t    2024-02-22 20:00:00-06:00\t5100.25\t5102.50\t5099.50\t5099.75\t4922\n",
        "4\t    2024-02-22 21:00:00-06:00\t5101.25\t5102.00\t5099.75\t5100.50\t4426\n",
        "\n",
        "The date time format corresponds to:\n",
        "\n",
        "- YYYY-MM-DD → The date (Year-Month-Day)\n",
        "- HH:MM:SS → The time in 24-hour format (Hour:Minute:Second)\n",
        "- 06:00 → The timezone offset from UTC, which in this case is UTC-6 (Central Time, since you set tz=\"America/Chicago\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6CZBqigaXMD"
      },
      "source": [
        "# Heatmap Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section stores constants for all sections that generate heatmap.\n",
        "There are values and labels for year, quarter, month, day, hour (+ day and night), horizon and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = [2023, 2024]\n",
        "\n",
        "days = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "}\n",
        "day_labels = [v for k,v in days.items()]\n",
        "\n",
        "weeks = {i: i+1 for i in range(13)}\n",
        "weeks_label = [v for k,v in weeks.items()]\n",
        "\n",
        "hours = {\n",
        "    0: \"12AM\",  1: \"1AM\",  2: \"2AM\",  3: \"3AM\",  4: \"4AM\",  5: \"5AM\",\n",
        "    6: \"6AM\",  7: \"7AM\",  8: \"8AM\",  9: \"9AM\",  10: \"10AM\", 11: \"11AM\",\n",
        "    12: \"12PM\", 13: \"1PM\", 14: \"2PM\", 15: \"3PM\", 16: \"4PM\", 17: \"5PM\",\n",
        "    18: \"6PM\", 19: \"7PM\", 20: \"8PM\", 21: \"9PM\", 22: \"10PM\", 23: \"11PM\"\n",
        "}\n",
        "\n",
        "hour_labels = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v']\n",
        "\n",
        "day_hours = {0: \"8AM\",  1: \"9AM\",  2: \"10AM\", 3: \"11AM\", 4: \"12PM\", 5: \"1PM\", 6: \"2PM\", 7: \"3PM\", 8: \"4PM\"}\n",
        "day_hours_labels = ['8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM']\n",
        "\n",
        "night_hours = {0: \"5PM\", 1: \"6PM\", 2: \"7PM\", 3: \"8PM\", 4: \"9PM\", 5: \"10PM\", 6: \"11PM\", 7: \"12AM\", 8: \"1AM\",  9: \"2AM\",  10: \"3AM\",  11: \"4AM\",  12: \"5AM\", 13: \"6AM\",  14: \"7AM\"}\n",
        "night_hours_labels = ['5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM']\n",
        "\n",
        "months = {i: i+1 for i in range(12)}\n",
        "months_label = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "\n",
        "quarters = {i: i+1 for i in range(4)}\n",
        "quarters_label = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "\n",
        "horizons = [0, 1, 2]\n",
        "horizons_label = [1, 2, 3]\n",
        "\n",
        "future = \"es\"\n",
        "models = {0: \"moirai\", 1: \"chronos\", 2: \"time_moe\"}\n",
        "models_label = [\"Moirai\", \"Chronos\", \"Time-MoE\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Toy function to understand how the precision is computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code is used to explain how do we compute the signs and the predictions. It is not used in practice, it is for demonstration only.\n",
        "Also, it doesn't use the TP, FP, TN, FN columns that were added in the dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "def precision_per_day(file_path, \n",
        "              target=\"Close\",\n",
        "              horizon=3, \n",
        "              date=None):\n",
        "\n",
        "    df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
        "    df.dropna(inplace=True)  # remove NaN values\n",
        "    # Force index to be in datetime format\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, utc=True)  # Convert to datetime, force UTC\n",
        "    # Convert from UTC to local time (assuming original data is in GMT-6)\n",
        "    df.index = df.index.tz_convert(\"America/Chicago\")  # Convert to Central Time\n",
        "            \n",
        "    df['Row_Position'] = range(len(df))  # Add original row position column\n",
        "\n",
        "    df_date = df.loc[date]  # Filter rows by date\n",
        "    \n",
        "    df_date_index_start = df_date.iloc[0][\"Row_Position\"]\n",
        "    df_date_index_end = df_date.iloc[-1][\"Row_Position\"]\n",
        "\n",
        "    # We take extra values (as many as there are horizons) to be able to predict the sign\n",
        "    # Correctly extract future timestamps from df.index\n",
        "    df_ground_truth_start_time = df.index[df_date_index_start + 1]\n",
        "    df_ground_truth_end_time = df.index[df_date_index_end + horizon]  # Prevent out-of-bounds\n",
        "\n",
        "    df_ground_truth = df.loc[df_ground_truth_start_time : df_ground_truth_end_time]\n",
        "\n",
        "    TP = [0 for _ in range(horizon)]\n",
        "    TN = [0 for _ in range(horizon)]\n",
        "    FP = [0 for _ in range(horizon)]\n",
        "    FN = [0 for _ in range(horizon)]\n",
        "\n",
        "\n",
        "    #print(\"\\n ---------- \\nDate: \", date)\n",
        "    #print(\"Ground truth start time: \", df_ground_truth_start_time)\n",
        "    #print(\"Ground truth end time: \", df_ground_truth_end_time)\n",
        "\n",
        "    # for each day\n",
        "    for index, row in df_date.iterrows():\n",
        "        #print(\"\\n\")\n",
        "        #print(\"Hour: \", index)\n",
        "        row_index = row[\"Row_Position\"]\n",
        "        base = row[target]\n",
        "        results_list = ast.literal_eval(row[\"Result\"])\n",
        "        \n",
        "        for horizon_index in range(1, horizon+1):\n",
        "            #print(\"Horizon n°\", horizon_index)\n",
        "            # locate the row + horizon_index \n",
        "            future_time = df.index[row_index + horizon_index]\n",
        "            y = df_ground_truth.loc[future_time, target]\n",
        "            prediction_result = results_list[horizon_index-1]\n",
        "            #print(\"Base: \", base, \"Real: \", y, \"Prediction: \", prediction_result)\n",
        "            sign_real_difference = np.sign(y - base)\n",
        "            sign_prediction_difference = np.sign(prediction_result - base)\n",
        "            #print(\"Real sign difference: \", sign_real_difference)\n",
        "            #print(\"Prediction sign difference: \", sign_prediction_difference)\n",
        "            \n",
        "            if sign_prediction_difference == 1 and sign_real_difference == 1:\n",
        "                TP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == -1:\n",
        "                TN[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == 1 and sign_real_difference == -1:\n",
        "                FP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == 1:\n",
        "                FN[horizon_index-1] += 1\n",
        "            else:\n",
        "                print(\"Error with signs. See the following values:\")\n",
        "                print(f\"Real: {sign_real_difference}, Prediction: {sign_prediction_difference}\")\n",
        "                print(f\"Real: {y}, Prediction: {prediction_result}\")\n",
        "    return TP, TN, FN, FP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function to compute the TP, FP, TN, FN for every row of one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def helper_metric(filepath: str):\n",
        "\n",
        "    df = pd.read_csv(f\"{filepath}.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "    # TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative\n",
        "    df[\"TP\"] = 0\n",
        "    df[\"TN\"] = 0\n",
        "    df[\"FP\"] = 0\n",
        "    df[\"FN\"] = 0\n",
        "\n",
        "    # We take the first 384 rows as context. We start predicting from the 385th row\n",
        "    first_predicted_row = 383\n",
        "    last_predicted_row = len(df) - 12\n",
        "    \n",
        "    for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "        base_price = row['Close']\n",
        "        future_predictions = ast.literal_eval(row[\"Result\"])\n",
        "        future_prices = df.loc[index:].iloc[1:13][\"Close\"].tolist() # get the next 12 values corresponding to the next 12 horizons\n",
        "        real_difference_signs = [np.sign(price - base_price) for price in future_prices]\n",
        "        predicted_difference_signs = [np.sign(prediction - base_price) for prediction in future_predictions]\n",
        "\n",
        "        TP = [0 for _ in range(12)]\n",
        "        TN = [0 for _ in range(12)]\n",
        "        FP = [0 for _ in range(12)]\n",
        "        FN = [0 for _ in range(12)]\n",
        "\n",
        "        # Compute the TP, TN, FP, FN for each horizon\n",
        "        for horizon_index, (real_difference_sign, predicted_difference_sign) in enumerate(zip(real_difference_signs, predicted_difference_signs)):\n",
        "            if real_difference_sign == predicted_difference_sign and real_difference_sign == 1:\n",
        "                TP[horizon_index] += 1\n",
        "            elif real_difference_sign == predicted_difference_sign and real_difference_sign == -1:\n",
        "                TN[horizon_index] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == 1:\n",
        "                FN[horizon_index] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == -1:\n",
        "                FP[horizon_index] += 1\n",
        "\n",
        "        # fill the column for TP, TN, FP, FN for the current row\n",
        "        df.at[index, \"TP\"] = str(TP)\n",
        "        df.at[index, \"TN\"] = str(TN)\n",
        "        df.at[index, \"FP\"] = str(FP)\n",
        "        df.at[index, \"FN\"] = str(FN)\n",
        "        \n",
        "    df.to_csv(f\"{filepath}_new_version.csv\", index=True)\n",
        "\n",
        "# Compute the TP, TN, FP, FN colulns for each model's dataframe\n",
        "for model_index, model_name in models.items():\n",
        "    helper_metric(filepath=f\"../future_data/es_future_final_{model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions to plot heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function creates an extended heatmap with averages for rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extended_heatmap_data(base_data, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Return an extended array that appends:\n",
        "      - A new last column with each row's ratio-of-sums\n",
        "      - A new last row with each column's ratio-of-sums\n",
        "      - The bottom-right cell with the ratio-of-total-sums\n",
        "    base_data: the cell-level precision\n",
        "    tp_data, fp_data: the sums of TPs and FPs for each cell\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = base_data.shape\n",
        "\n",
        "    # Prepare the extended array\n",
        "    extended = np.zeros((n_rows + 1, n_cols + 1), dtype=float)\n",
        "\n",
        "    # Fill in the main area\n",
        "    extended[:n_rows, :n_cols] = base_data\n",
        "\n",
        "    # Compute row-average via ratio of sums\n",
        "    for r in range(n_rows):\n",
        "        row_tp = np.sum(tp_data[r, :])\n",
        "        row_fp = np.sum(fp_data[r, :])\n",
        "        if row_tp + row_fp > 0:\n",
        "            extended[r, -1] = row_tp / (row_tp + row_fp)\n",
        "        else:\n",
        "            extended[r, -1] = 0.0\n",
        "\n",
        "    # Compute column-average via ratio of sums\n",
        "    for c in range(n_cols):\n",
        "        col_tp = np.sum(tp_data[:, c])\n",
        "        col_fp = np.sum(fp_data[:, c])\n",
        "        if col_tp + col_fp > 0:\n",
        "            extended[-1, c] = col_tp / (col_tp + col_fp)\n",
        "        else:\n",
        "            extended[-1, c] = 0.0\n",
        "\n",
        "    # Bottom-right cell: ratio of total TPs / total (TPs+FPs)\n",
        "    total_tp = np.sum(tp_data)\n",
        "    total_fp = np.sum(fp_data)\n",
        "    if total_tp + total_fp > 0:\n",
        "        extended[-1, -1] = total_tp / (total_tp + total_fp)\n",
        "    else:\n",
        "        extended[-1, -1] = 0.0\n",
        "\n",
        "    return extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def plot_extended_heatmap(extended_data, x_labels, y_labels, xlabel, ylabel, title,\n",
        "                          cmap=plt.cm.RdYlGn, norm_range=(0.65, 0.85), figsize=(14, 10)):\n",
        "    \"\"\"\n",
        "    Plot the heatmap given the extended data and labels.\n",
        "    The x_labels and y_labels should include the label for the extra (average) column/row.\n",
        "    \"\"\"\n",
        "\n",
        "    # Append 'Avg' labels for extra column and row\n",
        "    x_labels = x_labels + ['Avg']\n",
        "    y_labels = y_labels + ['Avg']\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    norm = plt.Normalize(*norm_range)\n",
        "    heatmap = plt.imshow(extended_data, cmap=cmap, norm=norm, aspect='auto')\n",
        "\n",
        "    # Colorbar with label\n",
        "    cbar = plt.colorbar(heatmap)\n",
        "    cbar.set_label('Mean Precision', fontsize=12)\n",
        "\n",
        "    # Adjust tick sizes\n",
        "    plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, fontsize=12, rotation=45, ha='right')\n",
        "    plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels, fontsize=12)\n",
        "\n",
        "    # Increase axis label and title font sizes\n",
        "    plt.xlabel(xlabel, fontsize=14)\n",
        "    plt.ylabel(ylabel, fontsize=14)\n",
        "    plt.title(title, fontsize=16)\n",
        "\n",
        "    # Annotate each cell with its numeric value\n",
        "    for i in range(extended_data.shape[0]):\n",
        "        for j in range(extended_data.shape[1]):\n",
        "            value = extended_data[i, j]\n",
        "            plt.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='black', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    output_path = f\"analysis/heatmaps/temporary_heatmaps/{title}.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Overwrite with a compressed version\n",
        "    Image.open(output_path).save(output_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision computation methods (for heatmaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from typing import List, Dict\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_heatmaps(x_values: Dict[int, str], y_values: Dict[int, str], horizons: List[int]) -> Dict[str, Dict[str, List[int]]]:\n",
        "\n",
        "    \"\"\"Initialize heatmaps dictionary to store TP and FP values.\"\"\"\n",
        "    return {\n",
        "        f\"{x_index}_{y_index}\": {\"TP\": [0] * len(horizons), \"FP\": [0] * len(horizons)}\n",
        "        for x_index, _ in enumerate(x_values) for y_index, _ in enumerate(y_values)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_data_matrices(len_x_values: int, len_y_values: int):\n",
        "    \"\"\"Initialize matrices to store TP, FP, and precision values.\"\"\"\n",
        "    return (\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # tp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # fp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float)   # precisions\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_data(model: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file for a given model.\"\"\"\n",
        "    return pd.read_csv(f\"analysis/future_data/es_future_final_{model}_updated.csv\", parse_dates=True, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_matrices(heatmaps, tp_data, fp_data, x_values, y_values):\n",
        "    \"\"\"Update TP and FP matrices based on heatmaps data.\"\"\"\n",
        "    for x_index, _ in enumerate(x_values):\n",
        "        for y_index, _ in enumerate(y_values):\n",
        "            key = f\"{x_index}_{y_index}\"\n",
        "            tp_data[y_index, x_index] = sum(heatmaps[key][\"TP\"])\n",
        "            fp_data[y_index, x_index] = sum(heatmaps[key][\"FP\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_precisions(precisions, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Fill precisions with precision = TP / (TP + FP).\n",
        "    This is a helper function to encapsulate the logic of computing precisions.\n",
        "    \"\"\"\n",
        "    len_x_values, len_y_values = precisions.shape\n",
        "    for x in range(len_x_values):\n",
        "        for y in range(len_y_values):\n",
        "            total = tp_data[x, y] + fp_data[x, y]\n",
        "            if total > 0:\n",
        "                precisions[x, y] = tp_data[x, y] / total\n",
        "            else:\n",
        "                precisions[x, y] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_hour_index(extracted_hour_label: str, day_hours_bool: bool, night_hours_bool: bool, all_hours_bool: bool) -> int:\n",
        "    \"\"\"\n",
        "    Find the index corresponding to the extracted hour label in the constant dictionaries defined above.\n",
        "\n",
        "    This function determines the index of a given hour label in one of three possible\n",
        "    dictionaries (`day_hours`, `night_hours`, or `hours`), depending on the provided\n",
        "    boolean flags.\n",
        "\n",
        "    Parameters:\n",
        "    - extracted_hour_label (str): The formatted hour string (e.g., \"3PM\").\n",
        "    - day_hours_bool (bool): Whether to look for the hour in `day_hours`.\n",
        "    - night_hours_bool (bool): Whether to look for the hour in `night_hours`.\n",
        "    - all_hours_bool (bool): Whether to look for the hour in the full `hours` dictionary.\n",
        "\n",
        "    Returns:\n",
        "    - int: The index of the hour in the selected dictionary, or -1 if not found.\n",
        "\n",
        "    Notes:\n",
        "    - The function prioritizes `day_hours`, then `night_hours`, and finally `all_hours`.\n",
        "    - If multiple flags are `True`, only the first matching condition applies.\n",
        "    \"\"\"\n",
        "    # Retrieve the index in day_hours, night_hours, or hours dictionaries.\n",
        "    hour_index = -1\n",
        "    if day_hours_bool and (extracted_hour_label in day_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in day_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif night_hours_bool and (extracted_hour_label in night_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in night_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif all_hours_bool and (extracted_hour_label in hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in hours.items() if hour_label == extracted_hour_label)\n",
        "    return hour_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision(models: List[str], x_values, y_values, x_labels, y_labels, xlabel, ylabel, title, day_hours_bool: bool, night_hours_bool: bool, all_hours_bool: bool, process_row, **kwargs):\n",
        "    \"\"\"Compute precision matrices for each model, fill precisions, and plot extended heatmap.\"\"\"\n",
        "\n",
        "    # 1. Initialize data structures\n",
        "    heatmaps = initialize_heatmaps(x_values, y_values, horizons)\n",
        "    tp_data, fp_data, precisions = initialize_data_matrices(len(x_values), len(y_values))\n",
        "\n",
        "    # 2. Collect TP and FP values from each model\n",
        "    for model_index, model in models.items():\n",
        "        df = load_model_data(model)\n",
        "        \n",
        "        # The context has a length of 384 hours and so ends at index 383. We start predicting at the 385th hour (at index 384). \n",
        "        # The last 12 hours of the datframe are not considered for prediction because they are used for comparison with the last 12 predictions (for the last hour of the set). \n",
        "        # We predict the next 12 hours for every row.\n",
        "        first_predicted_row, last_predicted_row = 383, len(df) - 12\n",
        "\n",
        "        for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "            kwargs[\"index\"], kwargs[\"model_index\"], kwargs[\"row\"], kwargs[\"heatmaps\"],kwargs[\"day_hours_bool\"], kwargs[\"night_hours_bool\"], kwargs[\"all_hours_bool\"] = index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool\n",
        "            process_row(**kwargs)\n",
        "\n",
        "    # 3. Aggregate results into tp_data and fp_data\n",
        "    update_matrices(heatmaps, tp_data, fp_data, x_values, y_values)\n",
        "\n",
        "    # 4. Fill precisions with the ratio = TP / (TP + FP)\n",
        "    fill_precisions(precisions, tp_data, fp_data)\n",
        "\n",
        "    # 5. Build extended heatmap data & plot\n",
        "    extended_data = create_extended_heatmap_data(precisions, tp_data, fp_data)\n",
        "    plot_extended_heatmap(\n",
        "        extended_data, \n",
        "        x_labels=x_labels,\n",
        "        y_labels=y_labels,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        title=title\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_days_hours(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for day and hour settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    \n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Determine the corresponding index of an hour in the dictionary storing all constants\n",
        "    hour_index = find_hour_index(extracted_hour_label, day_hours_bool, night_hours_bool, all_hours_bool)\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if (day in days.keys()):\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in horizons:\n",
        "                heatmaps[f\"{day}_{hour_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{day}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=hours, x_labels = day_labels, y_labels = hour_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row_day_hour=process_row_days_hours, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=day_hours, x_labels = day_labels, y_labels = day_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_days_hours, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=night_hours, x_labels = day_labels, y_labels = night_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_days_hours, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_models_hours(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for model and hour settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Determine the corresponding index of an hour in the dictionary storing all constants\n",
        "    hour_index = find_hour_index(extracted_hour_label, day_hours_bool, night_hours_bool, all_hours_bool)\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day in days.keys():\n",
        "        \n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=hours, x_labels = models_label, y_labels = hour_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_models_hours, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=day_hours, x_labels = models_label, y_labels = day_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_models_hours, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=night_hours, x_labels = models_label, y_labels = night_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_models_hours, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Horizon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_model_horizon(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for model and horizon settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "    \n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if (day in days.keys()):\n",
        "        \n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            \n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_model_horizon, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_model_horizon, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_model_horizon, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    if (day_index in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_days_models(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for day and model settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the hour directly\n",
        "    hour_index = index.hour\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_days_models, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_days_models, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_days_models, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Quarters x Months)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_quarters_models(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for quarters and models.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    - The data is grouped by quarter based on the month index.\n",
        "    - Months are categorized into quarters as follows:\n",
        "        - Q1: Jan (0), Feb (1), Mar (2)\n",
        "        - Q2: Apr (3), May (4), Jun (5)\n",
        "        - Q3: Jul (6), Aug (7), Sep (8)\n",
        "        - Q4: Oct (9), Nov (10), Dec (11)\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the month (1=January, 12=December) and determine the corresponding quarter (0=Q1, 3=Q4)\n",
        "    month_index = index.month\n",
        "    quarter_index = (month_index - 1) // 3\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "        \n",
        "        # Convert TP and FP values from string representations to lists\n",
        "        TP_list = ast.literal_eval(row[\"TP\"])\n",
        "        FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "        # Iterate over different forecast horizons and update heatmap values\n",
        "        for horizon_index in range(len(horizons)):\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=quarters, y_values={0: \"Month 1\", 1: \"Month 2\", 2: \"Month 3\"}, x_labels = quarters_label, y_labels = [\"Month 1\", \"Month 2\", \"Month 3\"], xlabel=\"Quarters\", ylabel=\"Months\", title=\"Mean Precision Heatmap (Quarters x Months)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_quarters_models, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mean Precision Heatmaps (Quarters x Weeks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_quarters_weeks(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for quarters and weeks.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    - Data is grouped into quarters and weeks:\n",
        "        - Weeks are indexed from 0 to 12 within each quarter.\n",
        "        - Quarters are determined based on the ISO calendar week number.\n",
        "    - The filtering logic ensures that only relevant trading hours are considered.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the hour directly\n",
        "    hour_index = index.hour\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Extract the ISO week number (1-52) and adjust to zero-based indexing\n",
        "    week_index = index.isocalendar()[1] - 1\n",
        "\n",
        "    # Determine the corresponding quarter and week within that quarter\n",
        "    quarter_index = week_index // 13  # Each quarter has 13 weeks\n",
        "    week_index = week_index % 13  # Keep week index within the quarter range\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"Mean Precision Heatmap (Quarters x Weeks)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_quarters_weeks, **kwargs)\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"Mean Precision Heatmap (Quarters x Weeks) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_quarters_weeks, **kwargs)\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"Mean Precision Heatmap (Quarters x Weeks) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_quarters_weeks, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cICsdMJU8B6r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "features = [\"ATR_10\", \"RSI\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "\n",
        "data = pd.read_csv(\"es_future_final_time_moe.csv\", parse_dates = True, index_col = 0)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and the column with the lists is named 'score'\n",
        "\n",
        "# Step 1: Convert the string representation of lists into actual lists\n",
        "data['score'] = data['score'].apply(ast.literal_eval)\n",
        "data['APE'] = data['APE'].apply(ast.literal_eval)\n",
        "data['SIGN'] = data['SIGN'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "# Step 2: Create a DataFrame from the lists and expand the columns\n",
        "score_df = pd.DataFrame(data['score'].tolist(), index=data.index)\n",
        "ape_df = pd.DataFrame(data['APE'].tolist(), index=data.index)\n",
        "sign_df = pd.DataFrame(data['SIGN'].tolist(), index=data.index)\n",
        "\n",
        "# Step 3: Rename the columns to SCORE_1, SCORE_2, ..., SCORE_12\n",
        "score_df.columns = [f'SCORE_{i+1}' for i in range(score_df.shape[1])]\n",
        "ape_df.columns = [f'APE_{i+1}' for i in range(ape_df.shape[1])]\n",
        "sign_df.columns = [f'SIGN_{i+1}' for i in range(sign_df.shape[1])]\n",
        "\n",
        "\n",
        "# Step 4: (Optional) Concatenate this new DataFrame with the original DataFrame\n",
        "data = pd.concat([data, score_df, ape_df, sign_df], axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLRXKcDS8iPd"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def beeswarm_plot(df, target, features):\n",
        "    \"\"\"\n",
        "    Beeswarm Plot with TreeSHAP\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing features and target\n",
        "        target (str): Name of the target column\n",
        "        features (list): List of feature column names\n",
        "\n",
        "    Returns:\n",
        "        shap_values: Computed SHAP values\n",
        "    \"\"\"\n",
        "    # Split data into features and target\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize the RandomForestRegressor\n",
        "    model = lgb.LGBMRegressor()\n",
        "    model.fit(X, y)\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "    # Compute SHAP values\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer(X_test)  # Preferred API in newer versions\n",
        "\n",
        "    # Global importance bar plot\n",
        "    shap.summary_plot(shap_values.values, X_test, plot_type=\"bar\")\n",
        "    plt.show()\n",
        "\n",
        "    # Beeswarm plot\n",
        "    shap.summary_plot(shap_values.values, X_test)\n",
        "    plt.show()\n",
        "\n",
        "    return shap_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uWfyWXqFTUZc"
      },
      "outputs": [],
      "source": [
        "df = data.copy()\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JW91heI18iXh"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_1\"] < df[\"APE_1\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_1\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYBFvEmp2Pm"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_1\"] < df[\"APE_1\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_1\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BatjPfY1cUnA"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_2\"] < df[\"APE_2\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_2\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bwW47bCp94k"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_2\"] < df[\"APE_2\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_2\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Ymey0xcg4l"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_3\"] < df[\"APE_3\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_3\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-7Riqh0qHjF"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_3\"] < df[\"APE_3\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_3\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKoCPc3CiJBB"
      },
      "outputs": [],
      "source": [
        "df[features].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZmIASAk8T0c"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"es_future_final_moirai.csv\").T, precision(\"es_future_final_moirai_moe.csv\").T, precision(\"es_future_final_time_moe.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3t3C6Td9LtS"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\").T, precision(\"gc_future_final_moirai_moe.csv\").T, precision(\"gc_future_final_time_moe.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AEzIrUn9jTg"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\").T, precision(\"btc_future_final_moirai.csv\").T, precision(\"btc_future_final_moirai.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5aC2deaS0C"
      },
      "source": [
        "# Buckets Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Model Precision Analysis by Feature Buckets**\n",
        "This notebook analyzes the precision of three different models (`moirai`, `chronos`, `time_moe`) by quantile buckets of selected features. The results are visualized in precision plots for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1) Load model data (only once) ---\n",
        "df_moirai = load_model_data(\"moirai\")\n",
        "df_chronos = load_model_data(\"chronos\")\n",
        "df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "# --- 2) Define common parameters ---\n",
        "first_predicted_row, last_predicted_row = 383, len(df_moirai) - 12\n",
        "features = [\"ATR_10\", \"RSI\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\", \"Volume\"]\n",
        "\n",
        "# For plotting\n",
        "horizons = [0, 2, 11]  # H=1, H=3, H=12\n",
        "horizon_labels = [\"H=1\", \"H=3\", \"H=12\"]\n",
        "horizon_styles = ['-', '--', ':']  # solid, dashed, dotted\n",
        "model_colors = {\"moirai\": \"blue\", \"chronos\": \"red\", \"time_moe\": \"green\"}\n",
        "ventile_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]  # \"0-5\", \"5-10\", ..., \"95-100\"\n",
        "\n",
        "# --- 3) Loop over each feature ---\n",
        "for feature in features:\n",
        "    # --- 3a) Compute the ventiles for this feature (using moirai data) ---\n",
        "    feature_ventile = df_moirai[feature].quantile([i / 20 for i in range(1, 20)])\n",
        "    \n",
        "    # --- 3b) Create the data structure for each model ---\n",
        "    models = {\n",
        "        \"moirai\": {\"df\": df_moirai, \"quantile_data\": []},\n",
        "        \"chronos\": {\"df\": df_chronos, \"quantile_data\": []},\n",
        "        \"time_moe\": {\"df\": df_time_moe, \"quantile_data\": []}\n",
        "    }\n",
        "    \n",
        "    # --- 3c) Initialize quantile buckets for each model ---\n",
        "    for model_name, model_data in models.items():\n",
        "        for i, value in enumerate(feature_ventile):\n",
        "            model_data[\"quantile_data\"].append({\n",
        "                'quantile': i + 1,\n",
        "                'value': value,\n",
        "                'TP_list': [0] * 12,  # 12 horizons\n",
        "                'FP_list': [0] * 12\n",
        "            })\n",
        "        # Add last bucket for values above the highest ventile\n",
        "        model_data[\"quantile_data\"].append({\n",
        "            'quantile': 20,\n",
        "            'value': float('inf'),\n",
        "            'TP_list': [0] * 12,\n",
        "            'FP_list': [0] * 12\n",
        "        })\n",
        "    \n",
        "    # --- 3d) Fill the quantile buckets with TP/FP values for each model ---\n",
        "    for model_name, model_data in models.items():\n",
        "        df_model = model_data[\"df\"]\n",
        "        \n",
        "        # Iterate over the predicted rows only\n",
        "        for index, row in df_model[first_predicted_row:last_predicted_row].iterrows():\n",
        "            feature_value = row[feature]\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            \n",
        "            # Find which quantile bucket this row's feature_value belongs to\n",
        "            for q in model_data[\"quantile_data\"]:\n",
        "                if feature_value <= q['value']:\n",
        "                    for i in range(12):  # 12 horizons\n",
        "                        q['TP_list'][i] += TP_list[i]\n",
        "                        q['FP_list'][i] += FP_list[i]\n",
        "                    break  # Stop after assigning to the first matching bucket\n",
        "    \n",
        "    # --- 3e) Plot precision by quantile bucket for each model and horizon ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for model_name, model_data in models.items():\n",
        "        for h_idx, horizon in enumerate(horizons):\n",
        "            precision_values = []\n",
        "            for q in model_data[\"quantile_data\"]:\n",
        "                TP = q['TP_list'][horizon]\n",
        "                FP = q['FP_list'][horizon]\n",
        "                precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                precision_values.append(precision)\n",
        "            \n",
        "            # Plot precision values\n",
        "            plt.plot(\n",
        "                range(len(precision_values)),\n",
        "                precision_values,\n",
        "                color=model_colors[model_name],\n",
        "                linestyle=horizon_styles[h_idx],\n",
        "                label=f\"{model_name} {horizon_labels[h_idx]}\"\n",
        "            )\n",
        "    \n",
        "    # Add horizontal lines for reference (optional)\n",
        "    plt.axhline(y=0.7, color='black', linestyle='--', alpha=0.7, label='Precision = 0.7')\n",
        "    plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, label='Precision = 0.8')\n",
        "    \n",
        "    # Set x-axis labels to ventile ranges\n",
        "    plt.xticks(range(20), ventile_labels, rotation=45)\n",
        "    plt.ylim(0.4, 1)\n",
        "    \n",
        "    plot_title = f\"ES Future - Precision per Buckets for {feature}\"\n",
        "    plt.xlabel('Feature Ventiles (%)')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(plot_title)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    file_path = f\"analysis/bucket_analysis_plots/temporary_buckets/{plot_title}.png\"\n",
        "    plt.savefig(file_path, dpi=300, bbox_inches=\"tight\")\n",
        "    Image.open(file_path).save(file_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "    \n",
        "    # Show the plot for the current feature\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bucket analysis for H=1, for the three Models and for all features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code is very similar with the one above. For clarity, the two codes are separated because the first one plot 7 graphs (one for each feature) with all three models on it. While this part focuses on plotting 3 graphs (one for each models) by fixing H=1 and the model. The two logics are different and need two separate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1) Load the data for all models\n",
        "df_moirai = load_model_data(\"moirai\")\n",
        "df_chronos = load_model_data(\"chronos\")\n",
        "df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "# 2) Define parameters\n",
        "first_predicted_row, last_predicted_row = 383, len(df_moirai) - 12\n",
        "features = [\"ATR_10\", \"RSI\", \"Volume\", \"DistanceToEMM20\", \n",
        "            \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "\n",
        "# We'll focus on H=1, which is index 0 in TP_list/FP_list\n",
        "horizon_index = 0\n",
        "\n",
        "# Keep a dictionary of model names to their DataFrame\n",
        "models = {\n",
        "    \"moirai\": df_moirai,\n",
        "    \"chronos\": df_chronos,\n",
        "    \"time_moe\": df_time_moe\n",
        "}\n",
        "\n",
        "# Each feature will get its own color\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "\n",
        "# For labeling x-axis with ventiles\n",
        "ventile_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "\n",
        "# 3) Create one figure per model\n",
        "for model_name, df_model in models.items():\n",
        "    \n",
        "    # Start a new figure for this model\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # For each feature, we will compute ventiles (using moirai's data, \n",
        "    # as in your advanced code) and plot on this model's figure\n",
        "    for f_idx, feature in enumerate(features):\n",
        "        \n",
        "        # --- a) Compute ventiles for this feature (based on df_moirai) ---\n",
        "        feature_ventiles = df_moirai[feature].quantile([i / 20 for i in range(1, 20)])\n",
        "        \n",
        "        # --- b) Initialize the quantile buckets (20 buckets) ---\n",
        "        quantile_data = []\n",
        "        for q_val in feature_ventiles:\n",
        "            quantile_data.append({'value': q_val, 'TP': 0, 'FP': 0})\n",
        "        # One extra bucket for values above the 95th percentile\n",
        "        quantile_data.append({'value': float('inf'), 'TP': 0, 'FP': 0})\n",
        "        \n",
        "        # --- c) Fill each bucket with TP/FP for horizon=1 ---\n",
        "        #     using the current model's DataFrame\n",
        "        for index, row in df_model[first_predicted_row:last_predicted_row].iterrows():\n",
        "            feature_value = row[feature]\n",
        "            \n",
        "            # Convert the \"TP\" and \"FP\" column from string to list\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            \n",
        "            # Find the correct bucket for this feature_value\n",
        "            for bucket in quantile_data:\n",
        "                if feature_value <= bucket['value']:\n",
        "                    bucket['TP'] += TP_list[horizon_index]  # H=1\n",
        "                    bucket['FP'] += FP_list[horizon_index]  # H=1\n",
        "                    break\n",
        "        \n",
        "        # --- d) Compute precision for each bucket ---\n",
        "        precision_values = []\n",
        "        for bucket in quantile_data:\n",
        "            tp = bucket['TP']\n",
        "            fp = bucket['FP']\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            precision_values.append(precision)\n",
        "        \n",
        "        # --- e) Plot the precision curve for this feature on this model ---\n",
        "        color = colors[f_idx]\n",
        "        label = f\"{feature} (H=1)\"\n",
        "        \n",
        "        plt.plot(\n",
        "            range(20),             # x-values: bucket indices 0..19\n",
        "            precision_values,      # y-values: computed precision\n",
        "            color=color,\n",
        "            label=label\n",
        "        )\n",
        "    \n",
        "    # 4) Finalize the plot for this model\n",
        "    plt.axhline(y=0.7, color='black', linestyle='--', alpha=0.7, label='Precision = 0.7')\n",
        "    plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, label='Precision = 0.8')\n",
        "    \n",
        "    plt.xticks(range(20), ventile_labels, rotation=45)\n",
        "    plt.ylim(0.4, 1)\n",
        "    plt.xlabel('Feature Ventiles (%)')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f\"Precision per Buckets (H=1) for {model_name.capitalize()}\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(loc='best', ncol=2)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "\n",
        "    file_path = f\"../bucket_analysis_plots/temporary_buckets/{model_name}_H1_Precision_Buckets_AllFeatures.png\"\n",
        "    plt.savefig(file_path, dpi=300, bbox_inches=\"tight\")\n",
        "    Image.open(file_path).save(file_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "    \n",
        "    # Show the figure for the current model\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scatter plot DistanceToEMA20, DistanceToEMA60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# Load the historical data\n",
        "df = pd.read_csv(\"data/ES=F.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# Compute and display the correlation matrix\n",
        "corr_value = df[['DistanceToEMM20', 'DistanceToEMM60']].corr(method='pearson')\n",
        "print(corr_value)\n",
        "\n",
        "# Scatter Plot (for comparison)\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.scatterplot(data=df, x='DistanceToEMM20', y='DistanceToEMM60', alpha=0.5)\n",
        "title = \"Scatter Plot DistanceToEMA20 vs. DistanceToEMA60\"\n",
        "plt.title(title)\n",
        "\n",
        "out_file = f\"analysis/scatter_plot/temporary_scatter_plot/{title}.png\"\n",
        "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
        "Image.open(out_file).save(out_file, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JNk9GmDiFGZd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
