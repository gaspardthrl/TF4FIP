{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Python version: python3.8 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i1zgg6wx4o1"
      },
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    'Volume', 'ROC_Volume',\n",
        "    'EMROC_Volume', 'ATR_10',\n",
        "    'RSI', 'DistanceToMM20', 'DistanceToMM60', 'DistanceToEMM20',\n",
        "    'DistanceToEMM60'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When fetching the ticker ES=F from Yahoo Finance, we obtain these columns:\n",
        "\n",
        "Price\t       Datetime\t             Close\t High\t  Low\t Open\tVolume\n",
        "0\t    2024-02-22 17:00:00-06:00\t5099.00\t5100.50\t5094.00\t5094.25\t0\n",
        "1\t    2024-02-22 18:00:00-06:00\t5095.75\t5099.25\t5092.25\t5098.75\t9420\n",
        "2\t    2024-02-22 19:00:00-06:00\t5099.50\t5101.75\t5095.00\t5095.75\t7390\n",
        "3\t    2024-02-22 20:00:00-06:00\t5100.25\t5102.50\t5099.50\t5099.75\t4922\n",
        "4\t    2024-02-22 21:00:00-06:00\t5101.25\t5102.00\t5099.75\t5100.50\t4426\n",
        "\n",
        "The date time format corresponds to:\n",
        "\n",
        "- YYYY-MM-DD → The date (Year-Month-Day)\n",
        "- HH:MM:SS → The time in 24-hour format (Hour:Minute:Second)\n",
        "- 06:00 → The timezone offset from UTC, which in this case is UTC-6 (Central Time, since you set tz=\"America/Chicago\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Q3aHyr8d6C"
      },
      "source": [
        "# Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKyAsuTCFKDc"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_features(df, feature_cols):\n",
        "    \"\"\"\n",
        "    Preprocess features with robust handling of outliers and infinite values\n",
        "    \"\"\"\n",
        "    features = df[feature_cols].select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "    # Replace infinite values with NaN\n",
        "    features = features.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Calculate robust statistics for each column\n",
        "    medians = features.median()\n",
        "    q1 = features.quantile(0.25)\n",
        "    q3 = features.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    # Replace outliers and NaN values with median\n",
        "    for col in features.columns:\n",
        "        mask = (features[col] < lower_bound[col]) | (features[col] > upper_bound[col]) | features[col].isna()\n",
        "        features.loc[mask, col] = medians[col]\n",
        "\n",
        "    return features\n",
        "\n",
        "def analyze_shap_values_with_beeswarm(df, feature_cols, target_cols):\n",
        "    \"\"\"\n",
        "    Analyze SHAP values and create beeswarm plots for multiple targets using DeepSHAP\n",
        "    With robust preprocessing and outlier handling\n",
        "    \"\"\"\n",
        "    print(\"TensorFlow GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "    # Robust preprocessing\n",
        "    print(\"Preprocessing features...\")\n",
        "    features = preprocess_features(df, feature_cols)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(features),\n",
        "        columns=features.columns,\n",
        "        index=features.index\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "    shap_values_dict = {}\n",
        "\n",
        "    for target in target_cols:\n",
        "        print(f\"\\nTraining model for target: {target}\")\n",
        "\n",
        "        # Prepare target variable with robust preprocessing\n",
        "        y = df[target].copy()\n",
        "        y = y.replace([np.inf, -np.inf], np.nan)\n",
        "        y_median = y.median()\n",
        "        y_q1 = y.quantile(0.25)\n",
        "        y_q3 = y.quantile(0.75)\n",
        "        y_iqr = y_q3 - y_q1\n",
        "        y_lower = y_q1 - 1.5 * y_iqr\n",
        "        y_upper = y_q3 + 1.5 * y_iqr\n",
        "        mask = (y < y_lower) | (y > y_upper) | y.isna()\n",
        "        y[mask] = y_median\n",
        "\n",
        "        # Standardize target\n",
        "        y_scaled = (y - y.mean()) / y.std()\n",
        "\n",
        "        # Build model\n",
        "        model = Sequential([\n",
        "            Dense(128, input_shape=(features.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "            Dense(32, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "\n",
        "        # Compile with smaller learning rate\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        # Train with early stopping\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        # Verify data is finite\n",
        "        assert np.all(np.isfinite(features_scaled)), \"Features contain non-finite values\"\n",
        "        assert np.all(np.isfinite(y_scaled)), \"Target contains non-finite values\"\n",
        "\n",
        "        print(f\"Feature stats - min: {features_scaled.min().min():.2f}, max: {features_scaled.max().max():.2f}\")\n",
        "        print(f\"Target stats - min: {y_scaled.min():.2f}, max: {y_scaled.max():.2f}\")\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            features_scaled,\n",
        "            y_scaled,\n",
        "            epochs=5,\n",
        "            batch_size=64,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
        "\n",
        "        try:\n",
        "            print(\"Calculating SHAP values...\")\n",
        "            # Use first 100 samples as background\n",
        "            background = features_scaled.iloc[:100]\n",
        "            explainer = shap.DeepExplainer(model, background)\n",
        "\n",
        "            # Calculate SHAP values in smaller batches\n",
        "            batch_size = 500\n",
        "            all_shap_values = []\n",
        "\n",
        "            for i in range(0, len(features_scaled), batch_size):\n",
        "                batch = features_scaled.iloc[i:i+batch_size]\n",
        "                batch_values = explainer.shap_values(batch)\n",
        "\n",
        "                if isinstance(batch_values, list):\n",
        "                    batch_values = batch_values[0]\n",
        "\n",
        "                all_shap_values.append(batch_values)\n",
        "\n",
        "            shap_values = np.vstack(all_shap_values)\n",
        "\n",
        "            # Store results\n",
        "            results[target] = pd.DataFrame({\n",
        "                'feature': feature_cols,\n",
        "                'importance': np.abs(shap_values).mean(axis=0)\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            shap_values_dict[target] = {\n",
        "                'values': shap_values,\n",
        "                'features': features_scaled\n",
        "            }\n",
        "\n",
        "            # Create and save plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.plots.beeswarm(\n",
        "                shap.Explanation(\n",
        "                    values=shap_values,\n",
        "                    data=features_scaled,\n",
        "                    feature_names=features_scaled.columns\n",
        "                ),\n",
        "                max_display=20\n",
        "            )\n",
        "            plt.title(f'SHAP Values Beeswarm Plot for {target}')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plot_filename = f\"SHAP/{target}_SHAP_beeswarm_plot.png\"\n",
        "            plt.savefig(plot_filename, bbox_inches='tight', dpi=300)\n",
        "            #plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {target}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return results, shap_values_dict\n",
        "\n",
        "# Example usage:\n",
        "# score_results, score_shap_values = analyze_shap_values_with_beeswarm(\n",
        "#     df,\n",
        "#     feature_cols=feature_cols,\n",
        "#     target_cols=['SIGN_1']\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fdnzlYXcZYTP"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAMgs9qJaymB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pip\n",
        "\n",
        "pip.main(['install', 'tensorflow==2.3.0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkhai5fOyaCv"
      },
      "outputs": [],
      "source": [
        "df = data.copy()\n",
        "for i in range(1, 2):\n",
        "  score_results, score_shap_values = analyze_shap_values_with_beeswarm(\n",
        "      df,\n",
        "      feature_cols=feature_cols,\n",
        "      target_cols=[f'SIGN_{i}']  # Using fewer targets for clarity\n",
        "  )\n",
        "\n",
        "\"\"\"\n",
        "# Similarly for SIGN and APE:\n",
        "sign_results = analyze_shap_values(df, feature_cols, [f'SIGN_{i}' for i in range(1, 13)])\n",
        "ape_results = analyze_shap_values(df, feature_cols, [f'APE_{i}' for i in range(1, 13)])\n",
        "\n",
        "plot_feature_importance(sign_results, 'SIGN')\n",
        "plt.show()\n",
        "plot_feature_importance(ape_results, 'APE')\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVbHOZpm4cs-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "\"\"\"\n",
        "max_sign_value = -float(\"inf\")\n",
        "min_sign_value = float(\"inf\")\n",
        "\n",
        "max_ape_value = -float(\"inf\")\n",
        "min_ape_value = float(\"inf\")\n",
        "\n",
        "max_score_value = -float(\"inf\")\n",
        "min_score_value = float(\"inf\")\n",
        "\"\"\"\n",
        "\n",
        "for target in targets:\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    \"\"\"\n",
        "    max_value = -float(\"inf\")\n",
        "    min_value = float(\"inf\")\n",
        "    \"\"\"\n",
        "\n",
        "    if \"SIGN\" in target:\n",
        "        max_value = max_sign_value\n",
        "        min_value = min_sign_value\n",
        "\n",
        "    elif \"APE\" in target:\n",
        "        max_value = max_ape_value\n",
        "        min_value = min_ape_value\n",
        "\n",
        "    elif \"SCORE\" in target:\n",
        "        max_value = max_score_value\n",
        "        min_value = min_score_value\n",
        "\n",
        "\n",
        "    for variable in variables:\n",
        "        df = data.copy()\n",
        "        df[f\"{variable}_bucket\"] = pd.qcut(df[variable], 10, labels=False)\n",
        "\n",
        "        # Calculate bucket means\n",
        "        bucket_means = df.groupby(f\"{variable}_bucket\")[target].mean().reset_index()\n",
        "\n",
        "        # Find the maximum and minimum values\n",
        "        \"\"\"\n",
        "        max_value = max(max_value, bucket_means[target].max())\n",
        "        min_value = min(min_value, bucket_means[target].min())\n",
        "        \"\"\"\n",
        "\n",
        "        # Plot each variable on the same plot\n",
        "        plt.plot(bucket_means[f\"{variable}_bucket\"],\n",
        "                 bucket_means[target],\n",
        "                 marker='o',\n",
        "                 label=variable)\n",
        "    \"\"\"\n",
        "    if target[-2:] != \"_1\":\n",
        "        if \"SIGN\" in target:\n",
        "            max_sign_value = max(max_sign_value, max_value)\n",
        "            min_sign_value = min(min_sign_value, min_value)\n",
        "\n",
        "        elif \"APE\" in target:\n",
        "            max_ape_value = max(max_ape_value, max_value)\n",
        "            min_ape_value = min(min_ape_value, min_value)\n",
        "\n",
        "        elif \"SCORE\" in target:\n",
        "            max_score_value = max(max_score_value, max_value)\n",
        "            min_score_value = min(min_score_value, min_value)\n",
        "    \"\"\"\n",
        "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "\n",
        "    if target[-2:] != \"_1\":\n",
        "        plt.ylim(min_value, max_value)\n",
        "\n",
        "\n",
        "    # Add plot details\n",
        "    plt.xlabel('Percentile Bucket')\n",
        "    plt.ylabel(f'Mean {target}')\n",
        "    plt.title(f'Mean {target} Across Different Variables per Percentile Bucket')\n",
        "    plt.legend(title='Variable', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plot_filename = f\"plots/{target}_grouped_percentile_plot.png\"\n",
        "    plt.savefig(plot_filename, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Grouped plot saved as {plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv5owlGnV7sB"
      },
      "outputs": [],
      "source": [
        "sign_lim = (min_sign_value, max_sign_value)\n",
        "ape_lim = (min_ape_value, max_ape_value)\n",
        "score_lim = (min_score_value, max_score_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Loq31FrWqPu"
      },
      "outputs": [],
      "source": [
        "sign_lim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpBD8qQEHT6U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Assuming 'data' is already defined with your features and target\n",
        "\n",
        "classifiers = dict()\n",
        "index = 1\n",
        "\n",
        "# Loop through all columns with 'SIGN_' to train models\n",
        "for col in data.columns:\n",
        "    if \"SIGN_\" in col:\n",
        "        X = data[['Hour_sin', 'Hour_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos',\n",
        "                  \"EMROC_Close\", \"EMROC_Volume\", \"ATR_10\", \"RSI\", 'DistanceToMM20',\n",
        "                  'DistanceToMM60', 'DistanceToEMM20', 'DistanceToEMM60']]\n",
        "        y = data[col]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize the RandomForestClassifier\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        classifiers[index] = {\n",
        "            \"model\": rf,\n",
        "            \"X_train\": X_train,\n",
        "            \"X_test\": X_test,\n",
        "            \"y_train\": y_train,\n",
        "            \"y_test\": y_test\n",
        "        }\n",
        "\n",
        "        index += 1\n",
        "\n",
        "# You can also check for other time-related correlations, such as Day or Week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2_2ag4HGfva"
      },
      "outputs": [],
      "source": [
        "! zip -r plots.zip plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbuLj-pODVXy"
      },
      "outputs": [],
      "source": [
        "predictions_dfs = dict()\n",
        "\n",
        "for key, values in classifiers.items():\n",
        "    value = list(values.values())\n",
        "    predictions = value[0].predict(value[2])\n",
        "    predictions_df = data[['Hour', 'Day', 'Week', 'Hour_sin', 'Hour_cos', 'Day_sin', 'Day_cos', 'Week_sin', 'Week_cos',\n",
        "                  \"EMROC_Close\", \"EMROC_Volume\", \"ATR_10\", \"RSI\", 'DistanceToMM20',\n",
        "                  'DistanceToMM60', 'DistanceToEMM20', 'DistanceToEMM60']].copy()\n",
        "\n",
        "    predictions_df = pd.concat([predictions_df, value[4]], axis=1)\n",
        "    predictions_df.dropna(inplace=True)\n",
        "    predictions_df[f\"SIGN_{key}_Prediction\"] = predictions\n",
        "    predictions_df[\"Result\"] = predictions_df[f\"SIGN_{key}_Prediction\"] == predictions_df[f\"SIGN_{key}\"]\n",
        "    predictions_dfs[key] = predictions_df\n",
        "    print(classification_report(value[4], value[0].predict(value[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I46NpIUbPfg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example: assuming Hour is [0..23], Day is [0..6], Week is a numeric week index.\n",
        "# If these are not the correct ranges or formats, adjust accordingly.\n",
        "\n",
        "for i, df in enumerate(predictions_dfs.values(), 1):\n",
        "    # Group and compute mean performance\n",
        "    hour_performance = df.groupby('Hour')['Result'].mean()  # Keep natural hourly order\n",
        "    day_performance = df.groupby('Day')['Result'].mean()    # Keep natural daily order\n",
        "    week_performance = df.groupby('Week')['Result'].mean()  # Keep natural weekly order\n",
        "\n",
        "    # Create a figure with three subplots\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
        "    fig.suptitle(f'Model Performance Patterns (DataFrame #{i})', fontsize=16, y=0.95)\n",
        "\n",
        "    # Hourly performance plot\n",
        "    sns.barplot(x=hour_performance.index, y=hour_performance.values, ax=axs[0], color=\"skyblue\")\n",
        "    axs[0].set_title('Mean Performance by Hour', fontsize=14)\n",
        "    axs[0].set_xlabel('Hour of the Day')\n",
        "    axs[0].set_ylabel('Mean Result')\n",
        "\n",
        "    # Daily performance plot\n",
        "    sns.barplot(x=day_performance.index, y=day_performance.values, ax=axs[1], color=\"lightgreen\")\n",
        "    axs[1].set_title('Mean Performance by Day', fontsize=14)\n",
        "    axs[1].set_xlabel('Day of the Week')\n",
        "    axs[1].set_ylabel('Mean Result')\n",
        "    # If day coding is known (e.g., 0=Monday), add custom labels:\n",
        "    axs[1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "\n",
        "    # Weekly performance plot\n",
        "    sns.barplot(x=week_performance.index, y=week_performance.values, ax=axs[2], color=\"salmon\")\n",
        "    axs[2].set_title('Mean Performance by Week', fontsize=14)\n",
        "    axs[2].set_xlabel('Week')\n",
        "    axs[2].set_ylabel('Mean Result')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyFcOK_nsXeh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Example DataFrame (replace with your data)\n",
        "df = pd.read_csv('results_BTC=F_384.csv')\n",
        "\n",
        "# Select the variable to segment by (e.g., 'ATR_10')\n",
        "variable = 'ATR_10'\n",
        "targets = ['score', 'APE', 'SIGN']\n",
        "\n",
        "# Step 1: Find dynamic thresholds using KMeans\n",
        "def get_thresholds(data, n_clusters=3):\n",
        "    valid_data = data.dropna().values.reshape(-1, 1)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    kmeans.fit(valid_data)\n",
        "    thresholds = sorted(kmeans.cluster_centers_.flatten())\n",
        "    return thresholds\n",
        "\n",
        "# Step 2: Create segments\n",
        "def segment_data(df, variable, thresholds):\n",
        "    df['segment'] = pd.cut(df[variable], bins=[-np.inf] + thresholds + [np.inf], labels=False)\n",
        "    return df\n",
        "\n",
        "# Step 3: Plot histograms for targets across segments\n",
        "def plot_histograms(df, targets):\n",
        "    for target in targets:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for segment in sorted(df['segment'].unique()):\n",
        "            subset = df[df['segment'] == segment]\n",
        "            plt.hist(subset[target].dropna(), alpha=0.5, label=f'Segment {segment}', bins=30)\n",
        "        plt.title(f'{target} Distribution Across {variable} Segments')\n",
        "        plt.xlabel(target)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Main execution\n",
        "if variable in df.columns:\n",
        "    thresholds = get_thresholds(df[variable])\n",
        "    df = segment_data(df, variable, thresholds)\n",
        "    plot_histograms(df, targets)\n",
        "else:\n",
        "    print(f\"Variable {variable} not found in DataFrame columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbQ3MoersZlT"
      },
      "outputs": [],
      "source": [
        "str(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icwvhAgcu5O4"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6CZBqigaXMD"
      },
      "source": [
        "# Precision Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section stores constants for all sections that generate heatmap.\n",
        "There are values and labels for year, quarter, month, day, hour (+ day and night), horizon and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################\n",
        "# Constants\n",
        "###############################################\n",
        "\n",
        "years = [2023, 2024]\n",
        "\n",
        "days = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "}\n",
        "day_labels = [v for k,v in days.items()]\n",
        "\n",
        "hours = {\n",
        "    0: \"12AM\",  1: \"1AM\",  2: \"2AM\",  3: \"3AM\",  4: \"4AM\",  5: \"5AM\",\n",
        "    6: \"6AM\",  7: \"7AM\",  8: \"8AM\",  9: \"9AM\",  10: \"10AM\", 11: \"11AM\",\n",
        "    12: \"12PM\", 13: \"1PM\", 14: \"2PM\", 15: \"3PM\", 16: \"4PM\", 17: \"5PM\",\n",
        "    18: \"6PM\", 19: \"7PM\", 20: \"8PM\", 21: \"9PM\", 22: \"10PM\", 23: \"11PM\"\n",
        "}\n",
        "\n",
        "hour_labels = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v']\n",
        "\n",
        "day_hours = {0: \"8AM\",  1: \"9AM\",  2: \"10AM\", 3: \"11AM\", 4: \"12PM\", 5: \"1PM\", 6: \"2PM\", 7: \"3PM\", 8: \"4PM\"}\n",
        "day_hours_labels = ['8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM']\n",
        "\n",
        "night_hours = {0: \"5PM\", 1: \"6PM\", 2: \"7PM\", 3: \"8PM\", 4: \"9PM\", 5: \"10PM\", 6: \"11PM\", 7: \"12AM\", 8: \"1AM\",  9: \"2AM\",  10: \"3AM\",  11: \"4AM\",  12: \"5AM\", 13: \"6AM\",  14: \"7AM\"}\n",
        "night_hours_labels = ['5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM']\n",
        "\n",
        "months = {i: i+1 for i in range(12)}\n",
        "months_label = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "\n",
        "quarters = [1, 2, 3, 4]\n",
        "quarters_label = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "\n",
        "horizons = [0, 1, 2]\n",
        "horizons_label = [1, 2, 3]\n",
        "\n",
        "future = \"es\"\n",
        "models = {0: \"moirai\", 1: \"chronos\", 2: \"time_moe\"}\n",
        "models_label = [\"Moirai\", \"Chronos\", \"Time-MoE\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision visualization function - Do not run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code is used to explain how do we compute the signs and the predictions. It is not used in practice, it is for demonstration only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "def precision_per_day(file_path, \n",
        "              target=\"Close\",\n",
        "              horizon=3, \n",
        "              date=None):\n",
        "\n",
        "    df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
        "    df.dropna(inplace=True)  # remove NaN values\n",
        "    # Force index to be in datetime format\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, utc=True)  # Convert to datetime, force UTC\n",
        "    # Convert from UTC to local time (assuming original data is in GMT-6)\n",
        "    df.index = df.index.tz_convert(\"America/Chicago\")  # Convert to Central Time\n",
        "            \n",
        "    df['Row_Position'] = range(len(df))  # Add original row position column\n",
        "\n",
        "    df_date = df.loc[date]  # Filter rows by date\n",
        "    \n",
        "    df_date_index_start = df_date.iloc[0][\"Row_Position\"]\n",
        "    df_date_index_end = df_date.iloc[-1][\"Row_Position\"]\n",
        "\n",
        "    # We take extra values (as many as there are horizons) to be able to predict the sign\n",
        "    # Correctly extract future timestamps from df.index\n",
        "    df_ground_truth_start_time = df.index[df_date_index_start + 1]\n",
        "    df_ground_truth_end_time = df.index[df_date_index_end + horizon]  # Prevent out-of-bounds\n",
        "\n",
        "    df_ground_truth = df.loc[df_ground_truth_start_time : df_ground_truth_end_time]\n",
        "\n",
        "    TP = [0 for _ in range(horizon)]\n",
        "    TN = [0 for _ in range(horizon)]\n",
        "    FP = [0 for _ in range(horizon)]\n",
        "    FN = [0 for _ in range(horizon)]\n",
        "\n",
        "\n",
        "    #print(\"\\n ---------- \\nDate: \", date)\n",
        "    #print(\"Ground truth start time: \", df_ground_truth_start_time)\n",
        "    #print(\"Ground truth end time: \", df_ground_truth_end_time)\n",
        "\n",
        "    # for each day\n",
        "    for index, row in df_date.iterrows():\n",
        "        #print(\"\\n\")\n",
        "        #print(\"Hour: \", index)\n",
        "        row_index = row[\"Row_Position\"]\n",
        "        base = row[target]\n",
        "        results_list = ast.literal_eval(row[\"Result\"])\n",
        "        \n",
        "        for horizon_index in range(1, horizon+1):\n",
        "            #print(\"Horizon n°\", horizon_index)\n",
        "            # locate the row + horizon_index \n",
        "            future_time = df.index[row_index + horizon_index]\n",
        "            y = df_ground_truth.loc[future_time, target]\n",
        "            prediction_result = results_list[horizon_index-1]\n",
        "            #print(\"Base: \", base, \"Real: \", y, \"Prediction: \", prediction_result)\n",
        "            sign_real_difference = np.sign(y - base)\n",
        "            sign_prediction_difference = np.sign(prediction_result - base)\n",
        "            #print(\"Real sign difference: \", sign_real_difference)\n",
        "            #print(\"Prediction sign difference: \", sign_prediction_difference)\n",
        "            \n",
        "            if sign_prediction_difference == 1 and sign_real_difference == 1:\n",
        "                TP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == -1:\n",
        "                TN[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == 1 and sign_real_difference == -1:\n",
        "                FP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == 1:\n",
        "                FN[horizon_index-1] += 1\n",
        "            else:\n",
        "                print(\"Error with signs. See the following values:\")\n",
        "                print(f\"Real: {sign_real_difference}, Prediction: {sign_prediction_difference}\")\n",
        "                print(f\"Real: {y}, Prediction: {prediction_result}\")\n",
        "    return TP, TN, FN, FP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function to compute the TP, FP, TN, FN for every row of the dataframe - Use only once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def helper_metric(filepath: str):\n",
        "\n",
        "    df = pd.read_csv(f\"{filepath}.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "    # Add 4 columns named \"TP\", \"TN\", \"FP\", \"FN\" to the dataframe\n",
        "    df[\"TP\"] = 0\n",
        "    df[\"TN\"] = 0\n",
        "    df[\"FP\"] = 0\n",
        "    df[\"FN\"] = 0\n",
        "\n",
        "    first_predicted_row = 383\n",
        "    last_predicted_row = len(df) - 12\n",
        "    \n",
        "    for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "        base_price = row['Close']\n",
        "        future_predictions = ast.literal_eval(row[\"Result\"])\n",
        "        future_prices = df.loc[index:].iloc[1:13][\"Close\"].tolist() # get the next 12 values corresponding to the next 12 horizons\n",
        "        real_difference_signs = [np.sign(price - base_price) for price in future_prices]\n",
        "        predicted_difference_signs = [np.sign(prediction - base_price) for prediction in future_predictions]\n",
        "\n",
        "        TP = [0 for _ in range(12)]\n",
        "        TN = [0 for _ in range(12)]\n",
        "        FP = [0 for _ in range(12)]\n",
        "        FN = [0 for _ in range(12)]\n",
        "\n",
        "        for horizon_index, (real_difference_sign, predicted_difference_sign) in enumerate(zip(real_difference_signs, predicted_difference_signs)):\n",
        "            if real_difference_sign == predicted_difference_sign and real_difference_sign == 1:\n",
        "                TP[horizon_index-1] += 1\n",
        "            elif real_difference_sign == predicted_difference_sign and real_difference_sign == -1:\n",
        "                TN[horizon_index-1] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == 1:\n",
        "                FN[horizon_index-1] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == -1:\n",
        "                FP[horizon_index-1] += 1\n",
        "\n",
        "        # fill the column for TP, TN, FP, FN for the current row\n",
        "        df.at[index, \"TP\"] = str(TP)\n",
        "        df.at[index, \"TN\"] = str(TN)\n",
        "        df.at[index, \"FP\"] = str(FP)\n",
        "        df.at[index, \"FN\"] = str(FN)\n",
        "        \n",
        "    df.to_csv(f\"{filepath}_updated.csv\", index=True)\n",
        "\n",
        "for model in models:\n",
        "    helper_metric(filepath=f\"../future_data/es_future_final_{model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions to plot heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function creates an extended heatmap with averages for rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extended_heatmap_data(base_data, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Return an extended array that appends:\n",
        "      - A new last column with each row's ratio-of-sums\n",
        "      - A new last row with each column's ratio-of-sums\n",
        "      - The bottom-right cell with the ratio-of-total-sums\n",
        "    base_data: the cell-level precision\n",
        "    tp_data, fp_data: the sums of TPs and FPs for each cell\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = base_data.shape\n",
        "\n",
        "    # Prepare the extended array\n",
        "    extended = np.zeros((n_rows + 1, n_cols + 1), dtype=float)\n",
        "\n",
        "    # Fill in the main area\n",
        "    extended[:n_rows, :n_cols] = base_data\n",
        "\n",
        "    # Compute row-average via ratio of sums\n",
        "    for r in range(n_rows):\n",
        "        row_tp = np.sum(tp_data[r, :])\n",
        "        row_fp = np.sum(fp_data[r, :])\n",
        "        if row_tp + row_fp > 0:\n",
        "            extended[r, -1] = row_tp / (row_tp + row_fp)\n",
        "        else:\n",
        "            extended[r, -1] = 0.0\n",
        "\n",
        "    # Compute column-average via ratio of sums\n",
        "    for c in range(n_cols):\n",
        "        col_tp = np.sum(tp_data[:, c])\n",
        "        col_fp = np.sum(fp_data[:, c])\n",
        "        if col_tp + col_fp > 0:\n",
        "            extended[-1, c] = col_tp / (col_tp + col_fp)\n",
        "        else:\n",
        "            extended[-1, c] = 0.0\n",
        "\n",
        "    # Bottom-right cell: ratio of total TPs / total (TPs+FPs)\n",
        "    total_tp = np.sum(tp_data)\n",
        "    total_fp = np.sum(fp_data)\n",
        "    if total_tp + total_fp > 0:\n",
        "        extended[-1, -1] = total_tp / (total_tp + total_fp)\n",
        "    else:\n",
        "        extended[-1, -1] = 0.0\n",
        "\n",
        "    return extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_extended_heatmap(extended_data, x_labels, y_labels, xlabel, ylabel, title,\n",
        "                          cmap=plt.cm.RdYlGn, norm_range=(0.45, 0.75), figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    Plot the heatmap given the extended data and labels.\n",
        "    The x_labels and y_labels should include the label for the extra (average) column/row.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an extra column and row for the averages\n",
        "    x_labels = x_labels + ['Avg']\n",
        "    y_labels = y_labels + ['Avg']\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    norm = plt.Normalize(*norm_range)\n",
        "    plt.imshow(extended_data, cmap=cmap, norm=norm, aspect='auto')\n",
        "    plt.colorbar(label='Mean Precision')\n",
        "    plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels)\n",
        "    plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "\n",
        "    # Annotate each cell with its numeric value\n",
        "    for i in range(extended_data.shape[0]):\n",
        "        for j in range(extended_data.shape[1]):\n",
        "            plt.text(j, i, f\"{extended_data[i, j]:.2f}\", ha='center', va='center', color='black')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"../temporary_heatmaps/{title}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final code to compute precision for all heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from typing import List, Dict\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_heatmaps(x_values: Dict[int, str], y_values: Dict[int, str], horizons: List[int]) -> Dict[str, Dict[str, List[int]]]:\n",
        "\n",
        "    \"\"\"Initialize heatmaps dictionary to store TP and FP values.\"\"\"\n",
        "    return {\n",
        "        f\"{x_index}_{y_index}\": {\"TP\": [0] * len(horizons), \"FP\": [0] * len(horizons)}\n",
        "        for x_index, _ in enumerate(x_values) for y_index, _ in enumerate(y_values)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_data_matrices(len_x_values: int, len_y_values: int):\n",
        "    \"\"\"Initialize matrices to store TP, FP, and precision values.\"\"\"\n",
        "    return (\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # tp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # fp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float)   # precisions\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_data(model: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file for a given model.\"\"\"\n",
        "    return pd.read_csv(f\"../future_data/es_future_final_{model}_updated.csv\", parse_dates=True, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_matrices(heatmaps, tp_data, fp_data, x_values, y_values):\n",
        "    \"\"\"Update TP and FP matrices based on heatmaps data.\"\"\"\n",
        "    for x_index, _ in enumerate(x_values):\n",
        "        for y_index, _ in enumerate(y_values):\n",
        "            key = f\"{x_index}_{y_index}\"\n",
        "            tp_data[y_index, x_index] = sum(heatmaps[key][\"TP\"])\n",
        "            fp_data[y_index, x_index] = sum(heatmaps[key][\"FP\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_precisions(precisions, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Fill precisions with precision = TP / (TP + FP).\n",
        "    This is a helper function to encapsulate the logic of computing precisions.\n",
        "    \"\"\"\n",
        "    len_x_values, len_y_values = precisions.shape\n",
        "    for x in range(len_x_values):\n",
        "        for y in range(len_y_values):\n",
        "            total = tp_data[x, y] + fp_data[x, y]\n",
        "            if total > 0:\n",
        "                precisions[x, y] = tp_data[x, y] / total\n",
        "            else:\n",
        "                precisions[x, y] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision(models: List[str], x_values, y_values, x_labels, y_labels, xlabel, ylabel, title, day_hours_bool: bool, night_hours_bool: bool, all_hours_bool: bool, process_row, **kwargs):\n",
        "    \"\"\"Compute precision matrices for each model, fill precisions, and plot extended heatmap.\"\"\"\n",
        "\n",
        "    # 1. Initialize data structures\n",
        "    heatmaps = initialize_heatmaps(x_values, y_values, horizons)\n",
        "    tp_data, fp_data, precisions = initialize_data_matrices(len(x_values), len(y_values))\n",
        "\n",
        "    # 2. Collect TP and FP values from each model\n",
        "    for model_index, model in models.items():\n",
        "        df = load_model_data(model)\n",
        "        \n",
        "        # The context has a length of 384 hours and so ends at index 383. We start predicting at the 385th hour (at index 384). \n",
        "        # The last 12 hours are not considered for prediction because the last 12 hours are used to compare the last 12 predictions for the last hour of the set. \n",
        "        # Remember that we predict the next 12 hours for every row.\n",
        "        first_predicted_row, last_predicted_row = 383, len(df) - 12\n",
        "\n",
        "        for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "            kwargs[\"index\"], kwargs[\"model_index\"], kwargs[\"row\"], kwargs[\"heatmaps\"],kwargs[\"day_hours_bool\"], kwargs[\"night_hours_bool\"], kwargs[\"all_hours_bool\"] = index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool\n",
        "            process_row(**kwargs)\n",
        "\n",
        "    # 3. Aggregate results into tp_data and fp_data\n",
        "    update_matrices(heatmaps, tp_data, fp_data, x_values, y_values)\n",
        "\n",
        "    # 4. Fill precisions with the ratio = TP / (TP + FP)\n",
        "    fill_precisions(precisions, tp_data, fp_data)\n",
        "\n",
        "    # 5. Build extended heatmap data & plot\n",
        "    extended_data = create_extended_heatmap_data(precisions, tp_data, fp_data)\n",
        "    plot_extended_heatmap(\n",
        "        extended_data, \n",
        "        x_labels=x_labels,\n",
        "        y_labels=y_labels,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        title=title\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    day = index.weekday()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Retrieve the index in day_hours, night_hours, or hours dictionaries.\n",
        "    if day_hours_bool and (extracted_hour_label in day_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in day_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif night_hours_bool and (extracted_hour_label in night_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in night_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif all_hours_bool and (extracted_hour_label in hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in hours.items() if hour_label == extracted_hour_label)\n",
        "\n",
        "    if (day in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "\n",
        "            for horizon_index in horizons:\n",
        "                heatmaps[f\"{day}_{hour_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{day}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=hours, x_labels = day_labels, y_labels = hour_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=day_hours, x_labels = day_labels, y_labels = day_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=night_hours, x_labels = day_labels, y_labels = night_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "    day = index.weekday()\n",
        "    \n",
        "    # Retrieve the index in day_hours, night_hours, or hours dictionaries.\n",
        "    if day_hours_bool and (extracted_hour_label in day_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in day_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif night_hours_bool and (extracted_hour_label in night_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in night_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif all_hours_bool and (extracted_hour_label in hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in hours.items() if hour_label == extracted_hour_label)\n",
        "\n",
        "    if (day in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=hours, x_labels = models_label, y_labels = hour_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=day_hours, x_labels = models_label, y_labels = day_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=night_hours, x_labels = models_label, y_labels = night_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Horizon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "    day = index.weekday()\n",
        "\n",
        "    if (day in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    if (day_index in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Months x Quarters) - TODO corriger ça"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##############################################\n",
        "\n",
        "# All models and full period\n",
        "\n",
        "##############################################\n",
        " \n",
        "import pandas as pd\n",
        "\n",
        "# Define start and end dates\n",
        "start_date = \"2023-03-01\"\n",
        "end_date = \"2024-12-12\" # we remove the last day because it was tedious to handle because of out-of-bound error\n",
        "\n",
        "# Generate a range of dates\n",
        "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
        "\n",
        "# store all results per month\n",
        "TP_sum_month = [0 for _ in range(12)]\n",
        "TN_sum_month = [0 for _ in range(12)]\n",
        "FP_sum_month = [0 for _ in range(12)]\n",
        "FN_sum_month = [0 for _ in range(12)]\n",
        "\n",
        "for date in date_range:\n",
        "    date_str = date.strftime(\"%Y-%m-%d\")\n",
        "    month = date.month - 1 # month is zero-based\n",
        "    for model in models:\n",
        "        (TP, TN, FN, FP) = precision_per_day(file_path=f\"../future_data/es_future_final_{model}.csv\", date=date_str)\n",
        "        # We sum the prediction for all horizons when we do sum(TP), sum(TN), sum(FN), sum(FP)\n",
        "        TP_sum_month[month] += sum(TP) \n",
        "        TN_sum_month[month] += sum(TN)\n",
        "        FN_sum_month[month] += sum(FN)\n",
        "        FP_sum_month[month] += sum(FP)\n",
        "\n",
        "        print(\"TP: \", TP_sum_month, \"TN: \", TN_sum_month, \"FP: \", FP_sum_month, \"FN: \", FN_sum_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TP_sum_month = [1292, 1438, 3141, 2719, 2637, 2897, 3035, 3356, 2909, 2900, 2704, 1980]\n",
        "# TN_sum_month = [2766, 1758, 3832, 4556, 4339, 3482, 4368, 4077, 4173, 4317, 4505, 2732]\n",
        "# FN_sum_month = [2119, 2639, 5124, 4523, 5262, 5551, 4678, 4459, 4423, 4933, 4406, 3876]\n",
        "# FP_sum_month = [441, 339, 1007, 1021, 971, 817, 1134, 1344, 1281, 1065, 1216, 529]\n",
        "\n",
        "heatmaps_month_quarter = dict()\n",
        "for month_index in range(12):\n",
        "    quarter = (month_index // 3) + 1\n",
        "    precision = TP_sum_month[month_index] / (TP_sum_month[month_index] + FP_sum_month[month_index])\n",
        "    heatmaps_month_quarter[f\"{month_index}_{quarter}\"] = precision\n",
        "\n",
        "print(heatmaps_month_quarter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "base_data = np.zeros((3, len(quarters)))\n",
        "for month_index in range(12):\n",
        "    quarter = (month_index) // 3 + 1\n",
        "    row_pos = (month_index) % 3\n",
        "    col_pos = quarter - 1\n",
        "    precision = heatmaps_month_quarter[f\"{month_index}_{quarter}\"]\n",
        "    base_data[row_pos, col_pos] = precision\n",
        "\n",
        "x_labels = quarters_label\n",
        "y_labels = [\"Month 1\", \"Month 2\", \"Month 3\"]\n",
        "\n",
        "plot_extended_heatmap(\n",
        "    create_extended_heatmap_data(base_data), \n",
        "    x_labels=x_labels,\n",
        "    y_labels=y_labels,\n",
        "    xlabel=\"Quarters\",\n",
        "    ylabel=\"Month-Position in Quarter\",\n",
        "    title=\"Mean Precision Heatmap (Months x Quarters)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2n_TwKXLTh"
      },
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cICsdMJU8B6r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "features = [\"ATR_10\", \"RSI\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "\n",
        "data = pd.read_csv(\"es_future_final_time_moe.csv\", parse_dates = True, index_col = 0)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and the column with the lists is named 'score'\n",
        "\n",
        "# Step 1: Convert the string representation of lists into actual lists\n",
        "data['score'] = data['score'].apply(ast.literal_eval)\n",
        "data['APE'] = data['APE'].apply(ast.literal_eval)\n",
        "data['SIGN'] = data['SIGN'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "# Step 2: Create a DataFrame from the lists and expand the columns\n",
        "score_df = pd.DataFrame(data['score'].tolist(), index=data.index)\n",
        "ape_df = pd.DataFrame(data['APE'].tolist(), index=data.index)\n",
        "sign_df = pd.DataFrame(data['SIGN'].tolist(), index=data.index)\n",
        "\n",
        "# Step 3: Rename the columns to SCORE_1, SCORE_2, ..., SCORE_12\n",
        "score_df.columns = [f'SCORE_{i+1}' for i in range(score_df.shape[1])]\n",
        "ape_df.columns = [f'APE_{i+1}' for i in range(ape_df.shape[1])]\n",
        "sign_df.columns = [f'SIGN_{i+1}' for i in range(sign_df.shape[1])]\n",
        "\n",
        "\n",
        "# Step 4: (Optional) Concatenate this new DataFrame with the original DataFrame\n",
        "data = pd.concat([data, score_df, ape_df, sign_df], axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLRXKcDS8iPd"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def beeswarm_plot(df, target, features):\n",
        "    \"\"\"\n",
        "    Beeswarm Plot with TreeSHAP\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing features and target\n",
        "        target (str): Name of the target column\n",
        "        features (list): List of feature column names\n",
        "\n",
        "    Returns:\n",
        "        shap_values: Computed SHAP values\n",
        "    \"\"\"\n",
        "    # Split data into features and target\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize the RandomForestRegressor\n",
        "    model = lgb.LGBMRegressor()\n",
        "    model.fit(X, y)\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "    # Compute SHAP values\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer(X_test)  # Preferred API in newer versions\n",
        "\n",
        "    # Global importance bar plot\n",
        "    shap.summary_plot(shap_values.values, X_test, plot_type=\"bar\")\n",
        "    plt.show()\n",
        "\n",
        "    # Beeswarm plot\n",
        "    shap.summary_plot(shap_values.values, X_test)\n",
        "    plt.show()\n",
        "\n",
        "    return shap_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uWfyWXqFTUZc"
      },
      "outputs": [],
      "source": [
        "df = data.copy()\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JW91heI18iXh"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_1\"] < df[\"APE_1\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_1\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFYBFvEmp2Pm"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_1\"] < df[\"APE_1\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_1\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BatjPfY1cUnA"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_2\"] < df[\"APE_2\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_2\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bwW47bCp94k"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_2\"] < df[\"APE_2\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_2\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Ymey0xcg4l"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_3\"] < df[\"APE_3\"].quantile(0.99)]\n",
        "values = beeswarm_plot(new_df, \"APE_3\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-7Riqh0qHjF"
      },
      "outputs": [],
      "source": [
        "new_df = df[df[\"APE_3\"] < df[\"APE_3\"].quantile(0.95)]\n",
        "values = beeswarm_plot(new_df, \"APE_3\", features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKoCPc3CiJBB"
      },
      "outputs": [],
      "source": [
        "df[features].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9vEgVsqiKcZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1tRUNAvXPhi"
      },
      "source": [
        "# Discarding features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZmIASAk8T0c"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"es_future_final_moirai.csv\").T, precision(\"es_future_final_moirai_moe.csv\").T, precision(\"es_future_final_time_moe.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"es_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"es_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"es_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"ES Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3t3C6Td9LtS"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\").T, precision(\"gc_future_final_moirai_moe.csv\").T, precision(\"gc_future_final_time_moe.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"gc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"gc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"GC Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AEzIrUn9jTg"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\").T, precision(\"btc_future_final_moirai.csv\").T, precision(\"btc_future_final_moirai.csv\").T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - All points - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.3).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q3 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.5).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q5 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")\n",
        "\n",
        "df = pd.concat([precision(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"btc_future_final_moirai_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T, precision(\"btc_future_final_time_moe.csv\", threshold_column=\"ATR_10\", quantile = 0.8).T], axis=1)\n",
        "df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "df.columns = [\"Moirai Base\", \"Moirai-MoE Small\", \"Time-MoE 200M\"]\n",
        "df.plot(\n",
        "    title=\"BTC Future - Discarding Q8 ATR_10 - Monday - 10AM-12PM\",\n",
        "    xlabel=\"Horizon\",\n",
        "    ylabel=\"Precision\",\n",
        "    ylim=(0, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5aC2deaS0C"
      },
      "source": [
        "# Buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcGvuL2QaRq4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def buckets(file_path, target = \"Close_denoised_standardized\", horizon = 12, threshold_column=None):\n",
        "    df = pd.read_csv(file_path, parse_dates = True, index_col = 0)\n",
        "    df[\"DistanceToEMA60\"] = df[\"DistanceToEMM60\"]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    def atr(target, horizon = 12, threshold_column = None, q1 = 0, q2 = 0.1):\n",
        "        i = 384\n",
        "        TP = [0 for k in range(horizon)]\n",
        "        TN = [0 for k in range(horizon)]\n",
        "        FP = [0 for k in range(horizon)]\n",
        "        FN = [0 for k in range(horizon)]\n",
        "        if threshold_column is not None:\n",
        "            low_threshold = df[threshold_column].quantile(q1)\n",
        "            high_threshold = df[threshold_column].quantile(q2)\n",
        "\n",
        "        # -13 is used to avoid predicting the next 12 hours for our last data point.\n",
        "        # Our last data point represents an hour. Starting the prediction at this hour is not possible because we could not verify if the prediction is correct or not. \n",
        "        while i < len(df)-13:\n",
        "            try:\n",
        "                index = df.index[i]\n",
        "                # Filter out all the rows that have a value in their threshold_column that is above the high_threshold or below the low_threshold.\n",
        "                if threshold_column is not None and (df[threshold_column].iloc[i] > high_threshold or df[threshold_column].iloc[i] < low_threshold):\n",
        "                    i += 1\n",
        "                    continue\n",
        "                # If the value in the threshold column is withing the range of the high and low thresholds, we proceed.\n",
        "                base = df[target].iloc[i] # we extract the value of the close denoised standardized at the current index.\n",
        "                signs = list()\n",
        "                for j in range(1, horizon+1):\n",
        "                    y = df[target].iloc[i+j]\n",
        "                    signs.append(int(np.sign(base-y)))\n",
        "                # we jump to the next hour\n",
        "                i+=1\n",
        "                pred_signs = ast.literal_eval(df[\"SIGN\"].iloc[i])\n",
        "                for j in range(horizon):\n",
        "                    if pred_signs[j] == 1 and signs[j] == -1:\n",
        "                        TN[j] += 1\n",
        "                    elif pred_signs[j] == 1:\n",
        "                        TP[j] += 1\n",
        "                    elif pred_signs[j] == -1 and signs[j] == -1:\n",
        "                        FP[j] += 1\n",
        "                    elif pred_signs[j] == -1:\n",
        "                        FN[j] += 1\n",
        "            except Exception as e:\n",
        "                print(i)\n",
        "                break\n",
        "        return list(map(lambda x: x[0]/(x[0]+x[1]), zip(TP, FP)))\n",
        "    \n",
        "    # compute the precision for each ventile\n",
        "    first_df = []\n",
        "    for ventile in range(20):\n",
        "        temp = pd.DataFrame(atr(target, horizon, threshold_column, q1 = ventile/20, q2 = (ventile+1)/20)).T\n",
        "        first_df.append(temp)\n",
        "    \n",
        "    # This dataframe is for the baseline\n",
        "    second_df = pd.DataFrame(atr(target, horizon)).T\n",
        "    final_df = pd.concat(first_df + [second_df], axis=0)\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bucket analysis for all features and all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "###############################################\n",
        "# Constants\n",
        "features = [\"ATR_10\", \"RSI\", \"Volume\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "\n",
        "###############################################\n",
        "# Define a function to process models with a given feature\n",
        "def process_model(file_name, feature):\n",
        "    df = buckets(file_name, horizon=12, threshold_column=feature)\n",
        "    df = df.iloc[:-1]  # Drop the baseline\n",
        "    df = df.T  # Transpose to have horizons as rows\n",
        "    df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "    df.columns = [f\"{feature} {i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "    return df.loc[1], df.loc[3], df.loc[12]  # Extract precision values for H1, H3, H12\n",
        "\n",
        "###############################################\n",
        "# Define a function to generate and save the plot\n",
        "def plot_feature(feature):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Process each model\n",
        "    models = [\n",
        "        (\"Moirai\", 'b', *process_model(\"es_future_final_moirai.csv\", feature)),\n",
        "        (\"Time-MoE\", 'g', *process_model(\"es_future_final_time_moe.csv\", feature)),\n",
        "        (\"Chronos\", 'r', *process_model(\"es_future_final_chronos.csv\", feature)),\n",
        "    ]\n",
        "    \n",
        "    horizons = [\"H=1\", \"H=3\", \"H=12\"]\n",
        "    linestyles = ['-', '--', ':']\n",
        "    bucket_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "    \n",
        "    for model_name, color, prec_H1, prec_H3, prec_H12 in models:\n",
        "        for horizon, prec, linestyle in zip(horizons, [prec_H1, prec_H3, prec_H12], linestyles):\n",
        "            plt.plot(bucket_labels, prec, linestyle=linestyle, color=color, label=f\"{model_name} {horizon}\")\n",
        "    \n",
        "    # Add horizontal reference lines at 0.5 and 0.6\n",
        "    plt.axhline(y=0.5, color='r', linestyle='--', label=\"Reference 0.5\")\n",
        "    plt.axhline(y=0.6, color='k', linestyle='--', label=\"Reference 0.6\")\n",
        "    \n",
        "    # Set fixed y-axis limits and y-ticks\n",
        "    plt.ylim(0.2, 0.75)\n",
        "    plt.yticks(np.arange(0.2, 0.8, 0.1))  # Set y-axis ticks from 0.2 to 0.7 with step 0.1\n",
        "    \n",
        "    plt.title(f\"ES Future - Precision per Buckets for {feature}\")\n",
        "    plt.xlabel(f\"Ventile for {feature}\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save the figure\n",
        "    file_name = f\"ES Future - Precision per Buckets for {feature}.png\"\n",
        "    plt.savefig(file_name, dpi=300, bbox_inches='tight')\n",
        "    \n",
        "    # Show the plot (optional, can be removed if only saving is needed)\n",
        "    plt.show()\n",
        "\n",
        "###############################################\n",
        "# Generate and save a plot for each feature\n",
        "for feature in features:\n",
        "    plot_feature(feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bucket analysis for H=1, Model=Time-MoE for all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "###############################################\n",
        "# Constants\n",
        "features = [\"ATR_10\", \"RSI\", \"Volume\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Assign unique colors to each feature\n",
        "\n",
        "###############################################\n",
        "# Define a function to process models with a given feature\n",
        "def process_model(file_name, feature):\n",
        "    df = buckets(file_name, horizon=12, threshold_column=feature)\n",
        "    df = df.iloc[:-1]  # Drop the baseline\n",
        "    df = df.T  # Transpose to have horizons as rows\n",
        "    df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "    df.columns = [f\"{feature} {i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "    return df.loc[1]  # Extract precision values for H=1 only\n",
        "\n",
        "###############################################\n",
        "# Generate a single plot for all features\n",
        "plt.figure(figsize=(12, 6))\n",
        "bucket_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "\n",
        "for feature, color in zip(features, colors):\n",
        "    prec_H1 = process_model(\"es_future_final_time_moe.csv\", feature)\n",
        "    plt.plot(bucket_labels, prec_H1, linestyle='-', color=color, label=f\"{feature} H=1\")\n",
        "\n",
        "plt.title(\"ES Future - Precision per Buckets for All Features (H=1 and Model=Time-MoE)\")\n",
        "plt.xlabel(\"Ventile\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "\n",
        "file_name = \"ES Future - Precision per Buckets for All Features (H=1 and Model=Time-MoE).png\"\n",
        "plt.savefig(file_name, dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We try to create a temporary column SIGN_PRIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD1W7hJ_QkU7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaBLJxdFDGOo"
      },
      "outputs": [],
      "source": [
        "esfuture_buckets_moirai_distance_to_ema_60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXsOCsnFRWfT"
      },
      "outputs": [],
      "source": [
        "# df = buckets(\"gc_future_final_moirai.csv\", threshold_column=\"ATR_10\").T\n",
        "# df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "# df.columns = [\"ATR_10 Q1\", \"ATR_10 Q1-Q2\", \"ATR_10 Q2-Q3\", \"ATR_10 Q3-Q4\", \"ATR_10 Q4-Q5\", \"ATR_10 Q5-Q6\", \"ATR_10 Q6-Q7\", \"ATR_10 Q7-Q8\", \"ATR_10 Q8-Q9\", \"ATR_10 Q9-Q10\"]\n",
        "# df.plot(\n",
        "#     title=\"GC Future - Precision per buckets - Monday - 10AM-12PM\",\n",
        "#     xlabel=\"Horizon\",\n",
        "#     ylabel=\"Precision\",\n",
        "#     figsize=(20, 10)\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nXSr5bLSpgl"
      },
      "outputs": [],
      "source": [
        "# df = buckets(\"btc_future_final_moirai.csv\", threshold_column=\"ATR_10\").T\n",
        "# df.index = pd.RangeIndex(start=1, stop=df.index.stop + 1, step=1)\n",
        "# df.columns = [\"ATR_10 Q1\", \"ATR_10 Q1-Q2\", \"ATR_10 Q2-Q3\", \"ATR_10 Q3-Q4\", \"ATR_10 Q4-Q5\", \"ATR_10 Q5-Q6\", \"ATR_10 Q6-Q7\", \"ATR_10 Q7-Q8\", \"ATR_10 Q8-Q9\", \"ATR_10 Q9-Q10\"]\n",
        "# df.plot(\n",
        "#     title=\"BTC Future - Precision per buckets - Monday - 10AM-12PM\",\n",
        "#     xlabel=\"Horizon\",\n",
        "#     ylabel=\"Precision\",\n",
        "#     figsize=(20, 10)\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKqVYqnsae70"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6xhxMgj99-5"
      },
      "outputs": [],
      "source": [
        "# Random forest Regressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "features = [\"ATR_10\", \"RSI\", \"Volume\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "target = \"Close_denoised\"\n",
        "\n",
        "def market_rf(file_path, features, target, horizon):\n",
        "    df = pd.read_csv(file_path, parse_dates = True, index_col = 0)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    X = df[features][:-3]\n",
        "    y = pd.concat([df[target][:-3], df[target].shift(-1)[:-3], df[target].shift(-2)[:-3], df[target].shift(-3)[:-3]], axis=1)\n",
        "    y.columns = [\"H=0\", \"H=1\", \"H=2\", \"H=3\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    rf1 = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=None,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf2 = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=None,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf3 = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features=None,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf1.fit(X_train, y_train[\"H=1\"])\n",
        "    rf2.fit(X_train, y_train[\"H=2\"])\n",
        "    rf3.fit(X_train, y_train[\"H=3\"])\n",
        "\n",
        "    y_pred1 = rf1.predict(X_test)\n",
        "    y_pred2 = rf2.predict(X_test)\n",
        "    y_pred3 = rf3.predict(X_test)\n",
        "\n",
        "    def get_metrics(y_test, y_pred):\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = mean_squared_error(y_test, y_pred)**0.5\n",
        "        print(f\"MAE: {mae}, R2: {r2}, MSE: {mse}, RMSE: {rmse}\")\n",
        "\n",
        "    print(\"H=1\")\n",
        "    get_metrics(y_test[\"H=1\"], y_pred1)\n",
        "    print(\"H=2\")\n",
        "    get_metrics(y_test[\"H=2\"], y_pred2)\n",
        "    print(\"H=3\")\n",
        "    get_metrics(y_test[\"H=3\"], y_pred3)\n",
        "    return y_test, y_pred1, y_pred2, y_pred3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLyy-fooHdmc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"es_future_final_time_moe.csv\", parse_dates = True, index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MridCOCaHiJD"
      },
      "outputs": [],
      "source": [
        "X = df[features][:-3]\n",
        "y = pd.concat([df[target][:-3], df[target].shift(-1)[:-3], df[target].shift(-2)[:-3], df[target].shift(-3)[:-3]], axis=1)\n",
        "y.columns = [\"H=0\", \"H=1\", \"H=2\", \"H=3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRuUubphDzkX"
      },
      "outputs": [],
      "source": [
        "y_test, y_pred1, y_pred2, y_pred3 = market_rf(\"es_future_final_moirai.csv\", features, target, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIDTTbp2mKFh"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(y_pred1, columns=[\"Predictions H=1\"])\n",
        "df2 = pd.DataFrame(y_pred2, columns=[\"Predictions H=2\"])\n",
        "df3 = pd.DataFrame(y_pred3, columns=[\"Predictions H=3\"])\n",
        "\n",
        "df1.index = y_test.index\n",
        "df2.index = y_test.index\n",
        "df3.index = y_test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Da6QE2Gcc7c"
      },
      "outputs": [],
      "source": [
        "rf_df = pd.concat([y_test, df1, df2, df3], axis=1)\n",
        "rf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SIrzD-Hs7kAj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"es_future_final_moirai.csv\", parse_dates = True, index_col = 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cww4HdzB7sMz"
      },
      "outputs": [],
      "source": [
        "aggregated = pd.concat([df, rf_df], axis=1)\n",
        "aggregated.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LicYzCzD82K3"
      },
      "outputs": [],
      "source": [
        "# ESFUTURE RF\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_test, y_pred1, y_pred2, y_pred3 = market_rf(\"es_future_final_moirai.csv\", features, target, 1)\n",
        "\n",
        "df1 = pd.DataFrame(y_pred1, columns=[\"Predictions H=1\"])\n",
        "df2 = pd.DataFrame(y_pred2, columns=[\"Predictions H=2\"])\n",
        "df3 = pd.DataFrame(y_pred3, columns=[\"Predictions H=3\"])\n",
        "\n",
        "df1.index = y_test.index\n",
        "df2.index = y_test.index\n",
        "df3.index = y_test.index\n",
        "\n",
        "rf_df = pd.concat([y_test, df1, df2, df3], axis=1)\n",
        "\n",
        "\n",
        "rf_df[\"Evolution 1\"] = rf_df[\"H=0\"] - rf_df[\"H=1\"]\n",
        "rf_df[\"Evolution 2\"] = rf_df[\"H=0\"] - rf_df[\"H=2\"]\n",
        "rf_df[\"Evolution 3\"] = rf_df[\"H=0\"] - rf_df[\"H=3\"]\n",
        "\n",
        "rf_df[\"Pred Evolution 1\"] = rf_df[\"H=0\"] - rf_df[\"Predictions H=1\"]\n",
        "rf_df[\"Pred Evolution 2\"] = rf_df[\"H=0\"] - rf_df[\"Predictions H=2\"]\n",
        "rf_df[\"Pred Evolution 3\"] = rf_df[\"H=0\"] - rf_df[\"Predictions H=3\"]\n",
        "\n",
        "rf_df[\"Evolution 1\"] = rf_df[\"Evolution 1\"].apply(np.sign)\n",
        "rf_df[\"Evolution 2\"] = rf_df[\"Evolution 2\"].apply(np.sign)\n",
        "rf_df[\"Evolution 3\"] = rf_df[\"Evolution 3\"].apply(np.sign)\n",
        "\n",
        "rf_df[\"Pred Evolution 1\"] = rf_df[\"Pred Evolution 1\"].apply(np.sign)\n",
        "rf_df[\"Pred Evolution 2\"] = rf_df[\"Pred Evolution 2\"].apply(np.sign)\n",
        "rf_df[\"Pred Evolution 3\"] = rf_df[\"Pred Evolution 3\"].apply(np.sign)\n",
        "\n",
        "es_future_moirai_rf = pd.concat([pd.read_csv(\"es_future_final_moirai.csv\", parse_dates = True, index_col = 0), rf_df], axis=1)\n",
        "es_future_moirai_moe_rf = pd.concat([pd.read_csv(\"es_future_final_moirai_moe.csv\", parse_dates = True, index_col = 0), rf_df], axis=1)\n",
        "es_future_time_moe_rf = pd.concat([pd.read_csv(\"es_future_final_time_moe.csv\", parse_dates = True, index_col = 0), rf_df], axis=1)\n",
        "\n",
        "es_future_moirai_rf.to_csv(\"es_future_moirai_rf.csv\")\n",
        "es_future_moirai_moe_rf.to_csv(\"es_future_moirai_moe_rf.csv\")\n",
        "es_future_time_moe_rf.to_csv(\"es_future_time_moe_rf.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scatter plot DistanceToEMA20, DistanceToEMA60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the historical data\n",
        "df = pd.read_csv(\"./data/ES=F.csv\", parse_dates=True, index_col=0)\n",
        "print(df[['DistanceToEMM20', 'DistanceToEMM60']])\n",
        "corr_value = df[['DistanceToEMM20', 'DistanceToEMM60']].corr(method='pearson')\n",
        "print(corr_value)  # correlation matrix for these two\n",
        "\n",
        "# Simple scatter plot\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(data=df, x='DistanceToEMM20', y='DistanceToEMM60')\n",
        "plt.title('Scatter plot DistanceToEMA20 vs. DistanceToEMA60')\n",
        "plt.show()\n",
        "\n",
        "# Or a correlation heatmap:\n",
        "sns.heatmap(corr_value, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JNk9GmDiFGZd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
